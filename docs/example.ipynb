{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Example usage\n",
                "\n",
                "To use `meerkat_dl` in a project:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.1.0\n"
                    ]
                }
            ],
            "source": [
                "import mdl\n",
                "\n",
                "print(mdl.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mdl.net.layer import LinearLayer\n",
                "import numpy as np\n",
                "\n",
                "from mdl.net.activation import ReLU\n",
                "from mdl.net.optimizers import GradientDescent\n",
                "from mdl.net.loss import MeanSquaredLoss\n",
                "from mdl.tensor import Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "num_samples = 5\n",
                "input_size = 5\n",
                "output_size = 1\n",
                "X = np.random.rand(num_samples, input_size).astype(np.float32)\n",
                "true_weights = np.random.rand(input_size, output_size).astype(np.float32)\n",
                "true_bias = np.random.rand(output_size).astype(np.float32)\n",
                "y = np.dot(X, true_weights) + true_bias"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_tensor = Tensor(X)\n",
                "y_tensor = Tensor(y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Tensor([[0.37454012 0.9507143  0.7319939  0.5986585  0.15601864]\n",
                            " [0.15599452 0.05808361 0.8661761  0.601115   0.7080726 ]\n",
                            " [0.02058449 0.96990985 0.83244264 0.21233912 0.18182497]\n",
                            " [0.1834045  0.30424225 0.52475643 0.43194503 0.29122913]\n",
                            " [0.6118529  0.13949387 0.29214466 0.36636186 0.45606998]])"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X_tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = LinearLayer(input_size, output_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_epochs = 100\n",
                "learning_rate = 0.01\n",
                "\n",
                "optimizer = GradientDescent(model.aggregate_parameters(as_list=True), learning_rate=learning_rate)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "activation_fn = ReLU()\n",
                "loss_fn = MeanSquaredLoss()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch [100/100], Loss: 1.200074553489685\n"
                    ]
                }
            ],
            "source": [
                "for epoch in range(num_epochs):\n",
                "    \n",
                "    pred = model(X_tensor)\n",
                "    activated_pred = activation_fn([pred])\n",
                "    \n",
                "    loss = loss_fn(activated_pred, y_tensor)\n",
                "    \n",
                "    loss.backward()\n",
                "    \n",
                "    optimizer.step()\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    if (epoch + 1) % 100 == 0:\n",
                "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.data}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def gradient_checker(tensor, epsilon=1e-5):\n",
                "    # Store the original tensor data\n",
                "    original_data = tensor.data.copy()\n",
                "\n",
                "    # Compute the gradients using automatic differentiation\n",
                "    tensor.backward(np.array(1.0))\n",
                "    autograd_grad = tensor.grad.copy()\n",
                "\n",
                "    # Reset the tensor data to the original values\n",
                "    tensor.data = original_data\n",
                "\n",
                "    # Initialize an array to store the numerical gradients\n",
                "    num_grad = np.zeros_like(original_data)\n",
                "\n",
                "    # Iterate over each element in the tensor and compute the numerical gradient\n",
                "    for idx in np.ndindex(original_data.shape):\n",
                "        # Perturb the current element by epsilon\n",
                "        tensor.data[idx] += epsilon\n",
                "\n",
                "        # Compute the forward pass with the perturbed tensor\n",
                "        perturbed_result = tensor.data.sum()  # Adjust as needed based on the operation\n",
                "\n",
                "        # Reset the tensor data to the original values\n",
                "        tensor.data = original_data\n",
                "\n",
                "        # Perturb the current element by -epsilon\n",
                "        tensor.data[idx] -= epsilon\n",
                "\n",
                "        # Compute the forward pass with the perturbed tensor\n",
                "        perturbed_result -= tensor.data.sum()  # Adjust as needed based on the operation\n",
                "\n",
                "        # Reset the tensor data to the original values\n",
                "        tensor.data = original_data\n",
                "\n",
                "        # Compute the numerical gradient for the current element\n",
                "        num_grad[idx] = perturbed_result / (2 * epsilon)\n",
                "\n",
                "    # Compute the relative error between autograd and numerical gradients\n",
                "    print(autograd_grad)\n",
                "    print(num_grad)\n",
                "    rel_error = np.linalg.norm(autograd_grad - num_grad) / np.linalg.norm(autograd_grad + num_grad)\n",
                "\n",
                "    return rel_error\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mdl.tensor import Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "deque([Tensor(172.0), Tensor([35. 56. 81.]), Tensor([5. 7. 9.]), Tensor([1. 2. 3.]), Tensor([4. 5. 6.]), Tensor([7. 8. 9.])])\n",
                        "Tensor(172.0)\n",
                        "Tensor([35. 56. 81.])\n",
                        "[Tensor(172.0)]\n",
                        "1.0\n",
                        "[0. 0. 0.]\n",
                        "[1. 1. 1.]\n",
                        "Tensor([5. 7. 9.])\n",
                        "[Tensor([35. 56. 81.])]\n",
                        "[1. 1. 1.]\n",
                        "[0. 0. 0.]\n",
                        "[7. 8. 9.]\n",
                        "Tensor([1. 2. 3.])\n",
                        "[Tensor([5. 7. 9.])]\n",
                        "[7. 8. 9.]\n",
                        "[0. 0. 0.]\n",
                        "[7. 8. 9.]\n",
                        "Tensor([4. 5. 6.])\n",
                        "[Tensor([5. 7. 9.])]\n",
                        "[7. 8. 9.]\n",
                        "[0. 0. 0.]\n",
                        "[7. 8. 9.]\n",
                        "Tensor([7. 8. 9.])\n",
                        "[Tensor([35. 56. 81.])]\n",
                        "[1. 1. 1.]\n",
                        "[0. 0. 0.]\n",
                        "[5. 7. 9.]\n"
                    ]
                },
                {
                    "ename": "Exception",
                    "evalue": "Shapes of gradient and Tensor need to match.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m result_sum\u001b[38;5;241m.\u001b[39mbackward(output_grad)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Verify gradients using the gradient checker\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m error_a \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m error_b \u001b[38;5;241m=\u001b[39m gradient_checker(tensor_b)\n\u001b[1;32m     18\u001b[0m error_c \u001b[38;5;241m=\u001b[39m gradient_checker(tensor_c)\n",
                        "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mgradient_checker\u001b[0;34m(tensor, epsilon)\u001b[0m\n\u001b[1;32m      5\u001b[0m original_data \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute the gradients using automatic differentiation\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m autograd_grad \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Reset the tensor data to the original values\u001b[39;00m\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/tensor.py:281\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, output_grad)\u001b[0m\n\u001b[1;32m    278\u001b[0m output_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_ndarray(output_grad)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m output_grad\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes of gradient and Tensor need to match.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate_grad(output_grad)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_dc_graph\u001b[38;5;241m.\u001b[39mbackpropogate(\u001b[38;5;28mself\u001b[39m)\n",
                        "\u001b[0;31mException\u001b[0m: Shapes of gradient and Tensor need to match."
                    ]
                }
            ],
            "source": [
                "# Complex scenario with multiple operations and backward pass\n",
                "tensor_a = Tensor(np.array([1, 2, 3]), requires_grad=True)\n",
                "tensor_b = Tensor(np.array([4, 5, 6]), requires_grad=True)\n",
                "tensor_c = Tensor(np.array([7, 8, 9]), requires_grad=True)\n",
                "\n",
                "# Operations\n",
                "result_add = tensor_a + tensor_b\n",
                "result_mul = result_add * tensor_c\n",
                "result_sum = result_mul.sum()\n",
                "\n",
                "# Simulating backward pass\n",
                "output_grad = np.array(1.0)\n",
                "result_sum.backward(output_grad)\n",
                "\n",
                "# Verify gradients using the gradient checker\n",
                "error_a = gradient_checker(tensor_a)\n",
                "error_b = gradient_checker(tensor_b)\n",
                "error_c = gradient_checker(tensor_c)\n",
                "\n",
                "# Check the errors\n",
                "print(f\"Gradient Checker Error for tensor_a: {error_a}\")\n",
                "print(f\"Gradient Checker Error for tensor_b: {error_b}\")\n",
                "print(f\"Gradient Checker Error for tensor_c: {error_c}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
