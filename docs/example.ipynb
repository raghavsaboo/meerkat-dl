{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Example usage\n",
                "\n",
                "To use `meerkat_dl` in a project:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.1.0\n"
                    ]
                }
            ],
            "source": [
                "import mdl\n",
                "\n",
                "print(mdl.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mdl.net.layer import LinearLayer\n",
                "import numpy as np\n",
                "\n",
                "from mdl.net.activation import ReLU\n",
                "from mdl.net.optimizers import GradientDescent\n",
                "from mdl.net.loss import MeanSquaredLoss\n",
                "from mdl.tensor import Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "num_samples = 1000\n",
                "input_size = 5\n",
                "output_size = 1\n",
                "X = np.random.rand(num_samples, input_size).astype(np.float32)\n",
                "true_weights = np.random.rand(input_size, output_size).astype(np.float32)\n",
                "true_bias = np.random.rand(output_size).astype(np.float32)\n",
                "y = np.dot(X, true_weights) + true_bias"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_tensor = Tensor(X)\n",
                "y_tensor = Tensor(y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = LinearLayer(input_size, output_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_epochs = 100\n",
                "learning_rate = 0.01\n",
                "\n",
                "optimizer = GradientDescent(model.aggregate_parameters(as_list=True), learning_rate=learning_rate)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "activation_fn = ReLU()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "shapes (1,5) and (1000,5) not aligned: 5 (dim 1) != 1000 (dim 0)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 3\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     activated_pred \u001b[38;5;241m=\u001b[39m activation_fn([pred])\n\u001b[1;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m MeanSquaredLoss(activated_pred, y_tensor)\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/net/layer.py:41\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, input_tensor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     37\u001b[0m     input_tensor: Tensor,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     40\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/net/layer.py:190\u001b[0m, in \u001b[0;36mLinearLayer.forward\u001b[0;34m(self, input_tensors)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensors: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/autodiff/operations.py:27\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, input_tensors, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     23\u001b[0m     input_tensors: List[Union[Tensor, Parameter]],\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Parameter]:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/autodiff/operations.py:77\u001b[0m, in \u001b[0;36mOperation.forward\u001b[0;34m(self, input_tensors, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m input_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_input_tensors(input_tensors)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_requires_grad(input_tensors)\n\u001b[0;32m---> 77\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/autodiff/linear.py:23\u001b[0m, in \u001b[0;36mLinear._forward\u001b[0;34m(self, input_tensors)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinear operation expects 1 input tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m input_tensors[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m output_data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m Tensor(output_data, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Add edges between output tensor and parameter tensors\u001b[39;00m\n",
                        "\u001b[0;31mValueError\u001b[0m: shapes (1,5) and (1000,5) not aligned: 5 (dim 1) != 1000 (dim 0)"
                    ]
                }
            ],
            "source": [
                "for epoch in range(num_epochs):\n",
                "    \n",
                "    pred = model([X_tensor])\n",
                "    activated_pred = activation_fn([pred])\n",
                "    \n",
                "    loss = MeanSquaredLoss(activated_pred, y_tensor)\n",
                "    \n",
                "    loss.backward()\n",
                "    \n",
                "    optimizer.step()\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    if (epoch + 1) % 100 == 0:\n",
                "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.data}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def gradient_checker(tensor, epsilon=1e-5):\n",
                "    # Store the original tensor data\n",
                "    original_data = tensor.data.copy()\n",
                "\n",
                "    # Compute the gradients using automatic differentiation\n",
                "    tensor.backward(np.array(1.0))\n",
                "    autograd_grad = tensor.grad.copy()\n",
                "\n",
                "    # Reset the tensor data to the original values\n",
                "    tensor.data = original_data\n",
                "\n",
                "    # Initialize an array to store the numerical gradients\n",
                "    num_grad = np.zeros_like(original_data)\n",
                "\n",
                "    # Iterate over each element in the tensor and compute the numerical gradient\n",
                "    for idx in np.ndindex(original_data.shape):\n",
                "        # Perturb the current element by epsilon\n",
                "        tensor.data[idx] += epsilon\n",
                "\n",
                "        # Compute the forward pass with the perturbed tensor\n",
                "        perturbed_result = tensor.data.sum()  # Adjust as needed based on the operation\n",
                "\n",
                "        # Reset the tensor data to the original values\n",
                "        tensor.data = original_data\n",
                "\n",
                "        # Perturb the current element by -epsilon\n",
                "        tensor.data[idx] -= epsilon\n",
                "\n",
                "        # Compute the forward pass with the perturbed tensor\n",
                "        perturbed_result -= tensor.data.sum()  # Adjust as needed based on the operation\n",
                "\n",
                "        # Reset the tensor data to the original values\n",
                "        tensor.data = original_data\n",
                "\n",
                "        # Compute the numerical gradient for the current element\n",
                "        num_grad[idx] = perturbed_result / (2 * epsilon)\n",
                "\n",
                "    # Compute the relative error between autograd and numerical gradients\n",
                "    print(autograd_grad)\n",
                "    print(num_grad)\n",
                "    rel_error = np.linalg.norm(autograd_grad - num_grad) / np.linalg.norm(autograd_grad + num_grad)\n",
                "\n",
                "    return rel_error\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mdl.tensor import Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "deque([Tensor(172.0), Tensor([35. 56. 81.]), Tensor([5. 7. 9.]), Tensor([1. 2. 3.]), Tensor([4. 5. 6.]), Tensor([7. 8. 9.])])\n",
                        "Tensor(172.0)\n",
                        "Tensor([35. 56. 81.])\n",
                        "[Tensor(172.0)]\n",
                        "1.0\n",
                        "[0. 0. 0.]\n",
                        "[1. 1. 1.]\n",
                        "Tensor([5. 7. 9.])\n",
                        "[Tensor([35. 56. 81.])]\n",
                        "[1. 1. 1.]\n",
                        "[0. 0. 0.]\n",
                        "[7. 8. 9.]\n",
                        "Tensor([1. 2. 3.])\n",
                        "[Tensor([5. 7. 9.])]\n",
                        "[7. 8. 9.]\n",
                        "[0. 0. 0.]\n",
                        "[7. 8. 9.]\n",
                        "Tensor([4. 5. 6.])\n",
                        "[Tensor([5. 7. 9.])]\n",
                        "[7. 8. 9.]\n",
                        "[0. 0. 0.]\n",
                        "[7. 8. 9.]\n",
                        "Tensor([7. 8. 9.])\n",
                        "[Tensor([35. 56. 81.])]\n",
                        "[1. 1. 1.]\n",
                        "[0. 0. 0.]\n",
                        "[5. 7. 9.]\n"
                    ]
                },
                {
                    "ename": "Exception",
                    "evalue": "Shapes of gradient and Tensor need to match.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m result_sum\u001b[38;5;241m.\u001b[39mbackward(output_grad)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Verify gradients using the gradient checker\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m error_a \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m error_b \u001b[38;5;241m=\u001b[39m gradient_checker(tensor_b)\n\u001b[1;32m     18\u001b[0m error_c \u001b[38;5;241m=\u001b[39m gradient_checker(tensor_c)\n",
                        "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mgradient_checker\u001b[0;34m(tensor, epsilon)\u001b[0m\n\u001b[1;32m      5\u001b[0m original_data \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute the gradients using automatic differentiation\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m autograd_grad \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Reset the tensor data to the original values\u001b[39;00m\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/tensor.py:281\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, output_grad)\u001b[0m\n\u001b[1;32m    278\u001b[0m output_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_ndarray(output_grad)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m output_grad\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes of gradient and Tensor need to match.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate_grad(output_grad)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_dc_graph\u001b[38;5;241m.\u001b[39mbackpropogate(\u001b[38;5;28mself\u001b[39m)\n",
                        "\u001b[0;31mException\u001b[0m: Shapes of gradient and Tensor need to match."
                    ]
                }
            ],
            "source": [
                "# Complex scenario with multiple operations and backward pass\n",
                "tensor_a = Tensor(np.array([1, 2, 3]), requires_grad=True)\n",
                "tensor_b = Tensor(np.array([4, 5, 6]), requires_grad=True)\n",
                "tensor_c = Tensor(np.array([7, 8, 9]), requires_grad=True)\n",
                "\n",
                "# Operations\n",
                "result_add = tensor_a + tensor_b\n",
                "result_mul = result_add * tensor_c\n",
                "result_sum = result_mul.sum()\n",
                "\n",
                "# Simulating backward pass\n",
                "output_grad = np.array(1.0)\n",
                "result_sum.backward(output_grad)\n",
                "\n",
                "# Verify gradients using the gradient checker\n",
                "error_a = gradient_checker(tensor_a)\n",
                "error_b = gradient_checker(tensor_b)\n",
                "error_c = gradient_checker(tensor_c)\n",
                "\n",
                "# Check the errors\n",
                "print(f\"Gradient Checker Error for tensor_a: {error_a}\")\n",
                "print(f\"Gradient Checker Error for tensor_b: {error_b}\")\n",
                "print(f\"Gradient Checker Error for tensor_c: {error_c}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
