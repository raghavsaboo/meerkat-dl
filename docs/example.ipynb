{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Example usage\n",
                "\n",
                "To use `meerkat_dl` in a project:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.1.0\n"
                    ]
                }
            ],
            "source": [
                "import mdl\n",
                "\n",
                "print(mdl.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mdl.net.layer import LinearLayer\n",
                "import numpy as np\n",
                "\n",
                "from mdl.net.activation import ReLU\n",
                "from mdl.net.optimizers import GradientDescent\n",
                "from mdl.net.loss import MeanSquaredErrorLoss\n",
                "from mdl.tensor import Tensor, Parameter\n",
                "from mdl.autodiff.dcgraph import DCGraph\n",
                "from mdl.autodiff.linear import Linear"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(2, 3)\n",
                        "(2, 3)\n",
                        "(2, 3)\n",
                        "(2, 3)\n",
                        "()\n"
                    ]
                }
            ],
            "source": [
                "input_a = Tensor(np.array([[1,2,3],[1,2,3]]), requires_grad=False)\n",
                "parameter_b = Tensor(np.array([[1,2,3],[1,2,3]]), requires_grad=True)\n",
                "target_c = Tensor(np.array([[3,4,5],[6,7,8]]), requires_grad=False)\n",
                "\n",
                "sum_fn = input_a + parameter_b\n",
                "abs_error = target_c - sum_fn\n",
                "sq_abs_error = abs_error ** 2\n",
                "sum_sq_abs_error = sq_abs_error.sum()\n",
                "mean_sum_sq_abs_error = sum_sq_abs_error / Tensor(sum_fn.shape[0])\n",
                "\n",
                "global_graph = DCGraph()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "backprop calculation\n",
                        "curr tensor: Tensor(15.5)\n",
                        "curr tensor shape: ()\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor(31.0)\n",
                        "curr tensor shape: ()\n",
                        "child tensor: Tensor(15.5)\n",
                        "child parent bshap: ()\n",
                        "unbroadcast axes to be summed: ()\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[ 1.  0.  1.]\n",
                        " [16.  9.  4.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "child tensor: Tensor(31.0)\n",
                        "child parent bshap: (2, 3)\n",
                        "unbroadcast axes to be summed: ()\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[ 1.  0. -1.]\n",
                        " [ 4.  3.  2.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "child tensor: Tensor([[ 1.  0.  1.]\n",
                        " [16.  9.  4.]])\n",
                        "child parent bshap: (2, 3)\n",
                        "unbroadcast axes to be summed: ()\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[3. 4. 5.]\n",
                        " [6. 7. 8.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[2. 4. 6.]\n",
                        " [2. 4. 6.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "child tensor: Tensor([[ 1.  0. -1.]\n",
                        " [ 4.  3.  2.]])\n",
                        "child parent bshap: (2, 3)\n",
                        "unbroadcast axes to be summed: ()\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[1. 2. 3.]\n",
                        " [1. 2. 3.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[1. 2. 3.]\n",
                        " [1. 2. 3.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "child tensor: Tensor([[2. 4. 6.]\n",
                        " [2. 4. 6.]])\n",
                        "child parent bshap: (2, 3)\n",
                        "unbroadcast axes to be summed: ()\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor(2.0)\n",
                        "curr tensor shape: ()\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "DCGraph({'_tensor_nodes': set()})"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mean_sum_sq_abs_error.backward()\n",
                "\n",
                "global_graph.reset_graph()\n",
                "\n",
                "global_graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "False\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[ 1.  0. -1.]\n",
                        " [ 4.  3.  2.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "child tensor: Tensor([[ 1.  0.  1.]\n",
                        " [16.  9.  4.]])\n",
                        "child parent bshap: (2, 3)\n",
                        "unbroadcast axes to be summed: ()\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[3. 4. 5.]\n",
                        " [6. 7. 8.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[2. 4. 6.]\n",
                        " [2. 4. 6.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "child tensor: Tensor([[ 1.  0. -1.]\n",
                        " [ 4.  3.  2.]])\n",
                        "child parent bshap: (2, 3)\n",
                        "unbroadcast axes to be summed: ()\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[1. 2. 3.]\n",
                        " [1. 2. 3.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[1. 2. 3.]\n",
                        " [1. 2. 3.]])\n",
                        "curr tensor shape: (2, 3)\n",
                        "child tensor: Tensor([[2. 4. 6.]\n",
                        " [2. 4. 6.]])\n",
                        "child parent bshap: (2, 3)\n",
                        "unbroadcast axes to be summed: ()\n"
                    ]
                }
            ],
            "source": [
                "print(abs_error in global_graph.tensor_nodes)\n",
                "abs_error.backward(np.array([[1.,1.,1.],[1.,1.,1.]]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "input tensor: Tensor([[0.92835057 0.00568409]])\n",
                        "target tensor: Tensor([[0.38320547]])\n"
                    ]
                }
            ],
            "source": [
                "input_size, output_size = 2, 1\n",
                "batch_size = 1\n",
                "\n",
                "# Create an instance of the Linear operation\n",
                "linear = Linear(input_size, output_size)\n",
                "\n",
                "# Generate random input tensor\n",
                "input_tensor = Tensor(np.random.rand(batch_size, input_size))\n",
                "print(f\"input tensor: {input_tensor}\")\n",
                "# Generate a random target tensor for loss calculation\n",
                "target_tensor = Tensor(np.random.rand(batch_size, output_size))\n",
                "print(f\"target tensor: {target_tensor}\")\n",
                "# Instantiate the real loss function from your framework\n",
                "loss_fn = MeanSquaredErrorLoss()  # Using the correct class name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(2, 2)\n",
                        "lin output: Tensor([[-0.70548725]])\n"
                    ]
                }
            ],
            "source": [
                "lin_output = linear(input_tensors=[input_tensor])\n",
                "\n",
                "print(f\"lin output: {lin_output}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "backprop calculation\n",
                        "curr tensor: Tensor([[-0.70548725]])\n",
                        "curr tensor shape: (1, 1)\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[0.92835057 0.00568409]])\n",
                        "curr tensor shape: (1, 2)\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[-0.76514035]\n",
                        " [ 0.84996504]])\n",
                        "curr tensor shape: (2, 1)\n",
                        "child tensor: Tensor([[-0.70548725]])\n",
                        "child parent bshap: (2, 2)\n",
                        "unbroadcast axes to be summed: (1,)\n",
                        "backprop calculation\n",
                        "curr tensor: Tensor([[0.]])\n",
                        "curr tensor shape: (1, 1)\n",
                        "child tensor: Tensor([[-0.70548725]])\n",
                        "child parent bshap: (2, 2)\n",
                        "unbroadcast axes to be summed: (0, 1)\n"
                    ]
                },
                {
                    "ename": "AxisError",
                    "evalue": "axis 1 is out of bounds for array of dimension 1",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlin_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/tensor.py:289\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, output_grad)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes of gradient and Tensor need to match.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate_grad(output_grad)\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_dc_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropogate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/autodiff/dcgraph.py:71\u001b[0m, in \u001b[0;36mDCGraph.backpropogate\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tensor_queue:\n\u001b[1;32m     70\u001b[0m     current \u001b[38;5;241m=\u001b[39m tensor_queue\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop_calculation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/tensor.py:310\u001b[0m, in \u001b[0;36mTensor.backprop_calculation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild tensor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild parent bshap: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild\u001b[38;5;241m.\u001b[39mparent_broadcast_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 310\u001b[0m local_grad \u001b[38;5;241m=\u001b[39m \u001b[43munbroadcast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_broadcast_shape\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m local_grad \u001b[38;5;241m=\u001b[39m local_grad\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate_grad(local_grad)\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/utilities.py:33\u001b[0m, in \u001b[0;36munbroadcast\u001b[0;34m(tensor, original_tensor_shape, broadcast_shape)\u001b[0m\n\u001b[1;32m     29\u001b[0m     axes_to_be_summed \u001b[38;5;241m=\u001b[39m get_axes_to_be_summed(\n\u001b[1;32m     30\u001b[0m         original_tensor_shape, broadcast_shape\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munbroadcast axes to be summed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxes_to_be_summed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     unbroadcasted_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes_to_be_summed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     unbroadcasted_tensor \u001b[38;5;241m=\u001b[39m tensor\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
                    ]
                }
            ],
            "source": [
                "lin_output.backward([[1.]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array(1.)"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mean_sum_sq_abs_error.grad\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(1, 3)\n",
                        "[[0. 0. 0.]]\n",
                        "[[0. 0. 0.]]\n",
                        "[[0. 0. 0.]]\n"
                    ]
                }
            ],
            "source": [
                "print(test_sum.parent_broadcast_shape)\n",
                "print(test_sum.grad)\n",
                "print(a.grad)\n",
                "print(b.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "()\n"
                    ]
                }
            ],
            "source": [
                "a.backward(output_grad=np.array([[1.,1.,1.]]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "()\n",
                        "()\n"
                    ]
                }
            ],
            "source": [
                "test_sum.backward(output_grad=np.array([[1.,1.,1.]]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "num_samples = 5\n",
                "input_size = 5\n",
                "output_size = 1\n",
                "X = np.random.rand(num_samples, input_size).astype(np.float32)\n",
                "true_weights = np.random.rand(input_size, output_size).astype(np.float32)\n",
                "true_bias = np.random.rand(output_size).astype(np.float32)\n",
                "y = np.dot(X, true_weights) + true_bias"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_tensor = Tensor(X)\n",
                "y_tensor = Tensor(y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Tensor([[0.37454012 0.9507143  0.7319939  0.5986585  0.15601864]\n",
                            " [0.15599452 0.05808361 0.8661761  0.601115   0.7080726 ]\n",
                            " [0.02058449 0.96990985 0.83244264 0.21233912 0.18182497]\n",
                            " [0.1834045  0.30424225 0.52475643 0.43194503 0.29122913]\n",
                            " [0.6118529  0.13949387 0.29214466 0.36636186 0.45606998]])"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X_tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = LinearLayer(input_size, output_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_epochs = 100\n",
                "learning_rate = 0.01\n",
                "\n",
                "optimizer = GradientDescent(model.aggregate_parameters_as_list(), learning_rate=learning_rate)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "activation_fn = ReLU()\n",
                "loss_fn = MeanSquaredErrorLoss()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch [100/100], Loss: 0.2983683943748474\n"
                    ]
                }
            ],
            "source": [
                "for epoch in range(num_epochs):\n",
                "    \n",
                "    pred = model(X_tensor)\n",
                "    activated_pred = activation_fn([pred])\n",
                "    \n",
                "    loss = loss_fn(activated_pred, y_tensor)\n",
                "    \n",
                "    loss.backward()\n",
                "    \n",
                "    optimizer.step()\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    if (epoch + 1) % 100 == 0:\n",
                "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.data}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def gradient_checker(tensor, epsilon=1e-5):\n",
                "    # Store the original tensor data\n",
                "    original_data = tensor.data.copy()\n",
                "\n",
                "    # Compute the gradients using automatic differentiation\n",
                "    tensor.backward(np.array(1.0))\n",
                "    autograd_grad = tensor.grad.copy()\n",
                "\n",
                "    # Reset the tensor data to the original values\n",
                "    tensor.data = original_data\n",
                "\n",
                "    # Initialize an array to store the numerical gradients\n",
                "    num_grad = np.zeros_like(original_data)\n",
                "\n",
                "    # Iterate over each element in the tensor and compute the numerical gradient\n",
                "    for idx in np.ndindex(original_data.shape):\n",
                "        # Perturb the current element by epsilon\n",
                "        tensor.data[idx] += epsilon\n",
                "\n",
                "        # Compute the forward pass with the perturbed tensor\n",
                "        perturbed_result = tensor.data.sum()  # Adjust as needed based on the operation\n",
                "\n",
                "        # Reset the tensor data to the original values\n",
                "        tensor.data = original_data\n",
                "\n",
                "        # Perturb the current element by -epsilon\n",
                "        tensor.data[idx] -= epsilon\n",
                "\n",
                "        # Compute the forward pass with the perturbed tensor\n",
                "        perturbed_result -= tensor.data.sum()  # Adjust as needed based on the operation\n",
                "\n",
                "        # Reset the tensor data to the original values\n",
                "        tensor.data = original_data\n",
                "\n",
                "        # Compute the numerical gradient for the current element\n",
                "        num_grad[idx] = perturbed_result / (2 * epsilon)\n",
                "\n",
                "    # Compute the relative error between autograd and numerical gradients\n",
                "    print(autograd_grad)\n",
                "    print(num_grad)\n",
                "    rel_error = np.linalg.norm(autograd_grad - num_grad) / np.linalg.norm(autograd_grad + num_grad)\n",
                "\n",
                "    return rel_error\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mdl.tensor import Tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "deque([Tensor(172.0), Tensor([35. 56. 81.]), Tensor([5. 7. 9.]), Tensor([1. 2. 3.]), Tensor([4. 5. 6.]), Tensor([7. 8. 9.])])\n",
                        "Tensor(172.0)\n",
                        "Tensor([35. 56. 81.])\n",
                        "[Tensor(172.0)]\n",
                        "1.0\n",
                        "[0. 0. 0.]\n",
                        "[1. 1. 1.]\n",
                        "Tensor([5. 7. 9.])\n",
                        "[Tensor([35. 56. 81.])]\n",
                        "[1. 1. 1.]\n",
                        "[0. 0. 0.]\n",
                        "[7. 8. 9.]\n",
                        "Tensor([1. 2. 3.])\n",
                        "[Tensor([5. 7. 9.])]\n",
                        "[7. 8. 9.]\n",
                        "[0. 0. 0.]\n",
                        "[7. 8. 9.]\n",
                        "Tensor([4. 5. 6.])\n",
                        "[Tensor([5. 7. 9.])]\n",
                        "[7. 8. 9.]\n",
                        "[0. 0. 0.]\n",
                        "[7. 8. 9.]\n",
                        "Tensor([7. 8. 9.])\n",
                        "[Tensor([35. 56. 81.])]\n",
                        "[1. 1. 1.]\n",
                        "[0. 0. 0.]\n",
                        "[5. 7. 9.]\n"
                    ]
                },
                {
                    "ename": "Exception",
                    "evalue": "Shapes of gradient and Tensor need to match.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m result_sum\u001b[38;5;241m.\u001b[39mbackward(output_grad)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Verify gradients using the gradient checker\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m error_a \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m error_b \u001b[38;5;241m=\u001b[39m gradient_checker(tensor_b)\n\u001b[1;32m     18\u001b[0m error_c \u001b[38;5;241m=\u001b[39m gradient_checker(tensor_c)\n",
                        "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mgradient_checker\u001b[0;34m(tensor, epsilon)\u001b[0m\n\u001b[1;32m      5\u001b[0m original_data \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute the gradients using automatic differentiation\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m autograd_grad \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Reset the tensor data to the original values\u001b[39;00m\n",
                        "File \u001b[0;32m~/Projects/meerkat_dl/src/mdl/tensor.py:281\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, output_grad)\u001b[0m\n\u001b[1;32m    278\u001b[0m output_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_ndarray(output_grad)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m output_grad\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes of gradient and Tensor need to match.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate_grad(output_grad)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_dc_graph\u001b[38;5;241m.\u001b[39mbackpropogate(\u001b[38;5;28mself\u001b[39m)\n",
                        "\u001b[0;31mException\u001b[0m: Shapes of gradient and Tensor need to match."
                    ]
                }
            ],
            "source": [
                "# Complex scenario with multiple operations and backward pass\n",
                "tensor_a = Tensor(np.array([1, 2, 3]), requires_grad=True)\n",
                "tensor_b = Tensor(np.array([4, 5, 6]), requires_grad=True)\n",
                "tensor_c = Tensor(np.array([7, 8, 9]), requires_grad=True)\n",
                "\n",
                "# Operations\n",
                "result_add = tensor_a + tensor_b\n",
                "result_mul = result_add * tensor_c\n",
                "result_sum = result_mul.sum()\n",
                "\n",
                "# Simulating backward pass\n",
                "output_grad = np.array(1.0)\n",
                "result_sum.backward(output_grad)\n",
                "\n",
                "# Verify gradients using the gradient checker\n",
                "error_a = gradient_checker(tensor_a)\n",
                "error_b = gradient_checker(tensor_b)\n",
                "error_c = gradient_checker(tensor_c)\n",
                "\n",
                "# Check the errors\n",
                "print(f\"Gradient Checker Error for tensor_a: {error_a}\")\n",
                "print(f\"Gradient Checker Error for tensor_b: {error_b}\")\n",
                "print(f\"Gradient Checker Error for tensor_c: {error_c}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
